# kenna-bulk-custom-field-update
require 'rest-client'
require 'json'
require 'csv'

#These are the arguments we are expecting to get 
@token = ARGV[0]
@assets_per_query = ARGV[1] #number of assets to update at one time to keep the vuln pull under 20 pages
@report_filename = ARGV[2] #name of the resulting csv file
@include_details = ARGV[3] #set to true only if your Kenna instance is enabled for vuln detail extract
ARGV.length > 4 ? @risk_meter_id = ARGV[4] : @risk_meter_id = nil #risk meter from which to pull vulns
ARGV.length > 5 ? @q_param = ARGV[5] : @q_param = nil
ARGV.length > 6 ? @base_url = ARGV[6] : @base_url = "https://api.kennasecurity.com/"

#Variables we'll need later
@vuln_url = "#{@base_url}vulnerabilities/search?per_page=5000"
@rm_url = "#{@base_url}asset_groups/"
@data_exports_url = "#{@base_url}data_exports"
@headers = {'Content-type' => 'application/json', 'X-Risk-Token' => @token }

@max_retries = 5

start_time = Time.now
@output_filename = Logger.new("csv-extract-#{start_time.strftime("%Y%m%dT%H%M")}.txt")
@debug = false


# Encoding characters
csv_headers = 
  [
    "id",
    "status",
    "cve id",
    "score",
    "cve description",
    "created",
    "last seen",
    "QIDs",
    "fix id",
    "cvss threat",
    "cvss severity",
    "active breach",
    "malware exploitable",
    "easily exploitable",
    "predicted exploitable",
    "patch available",
    "patch published",
    "hostname",
    "ip",
    "os",
    "tags"
  ]

csv_headers << "details" if @include_details

def get_data(get_url)
  puts "starting query" if @debug
  puts "get data url = #{get_url}" if @debug
  query_return = ""
  begin
    query_return = RestClient::Request.execute(
      :method => :get,
      :url => get_url,
      :headers => @headers
    )
  rescue RestClient::TooManyRequests =>e
   retry
  rescue RestClient::UnprocessableEntity => e
    puts "unprocessible entity: #{e.message}"
  rescue RestClient::BadRequest => e
    @output_filename.error("Rest client BadRequest: #{get_url}...#{e.message} (time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
    puts "BadRequest: #{e.backtrace.inspect}"
  rescue RestClient::Exception => e
    @retries ||= 0
    if @retries < @max_retries
      @retries += 1
      sleep(15)
      retry
    else
      @output_filename.error("General RestClient error #{get_url}... #{e.message}(time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
      puts "Unable to get vulns: #{e.backtrace.inspect}"
    end
  rescue Exception => e
    @output_filename.error("General Exception: #{get_url}...#{e.message} (time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
    puts "BadRequest: #{e.backtrace.inspect}"
  end
  return query_return
end

def get_bulk_assets()
  puts "starting bulk query" if @debug
  bulk_query_json_string = "{\"status\": [\"active\"],"
  bulk_query_json_string = bulk_query_json_string + " \"search_id\": \"#{@risk_meter_id}\"," if !@risk_meter_id.nil? && !@risk_meter_id.empty?
  bulk_query_json_string = bulk_query_json_string + " \"export_settings\": { \"format\": \"json\", "
  bulk_query_json_string = bulk_query_json_string + "\"compression\": \"gzip\", \"model\": \"asset\" }}"

  bulk_query_json = JSON.parse(bulk_query_json_string)

  puts bulk_query_json.to_s if @debug
  begin
    query_response = RestClient::Request.execute(
      :method => :post,
      :url => @data_exports_url,
      :headers => @headers,
      :payload => bulk_query_json
    ) 
    puts query_response if @debug
    query_response_json = JSON.parse(query_response.body)
    searchID = query_response_json.fetch("search_id")
    #searchID = 1079331
    puts "searchID = #{searchID}" if @debug
    output_results = "myoutputfile_#{searchID}.gz"
    searchComplete = false

    while searchComplete == false
      
      status_code = RestClient.get("#{@data_exports_url}/status?search_id=#{searchID}", @headers).code

      puts "status code =#{status_code}" if @debug
      if status_code != 200 then 
        puts "sleeping for async query" if @debug
        sleep(60)
        next
      else
        puts "ansyc query complete" if @debug
        searchComplete = true
        File.open(output_results, 'w') {|f|
          block = proc { |response|
            response.read_body do |chunk| 
              f.write chunk
            end
          }
          RestClient::Request.new(:method => :get, :url => "#{@data_exports_url}?search_id=#{searchID}", :headers => @headers, :block_response => block).execute
        }
        gzfile = open(output_results)
        gz = Zlib::GzipReader.new(gzfile)
        json_data = JSON.parse(gz.read)["assets"]
      end
    end
  rescue RestClient::TooManyRequests => e
    retry
  rescue RestClient::UnprocessableEntity => e
    puts "unprocessible entity: #{e.message}"
  rescue RestClient::BadRequest => e
    @output_filename.error("Rest client BadRequest:...#{e.message} (time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
    puts "BadRequest: #{e.backtrace.inspect}"
  rescue RestClient::Exception => e
    @retries ||= 0
    if @retries < @max_retries
      @retries += 1
      sleep(15)
      retry
    else
      @output_filename.error("General RestClient error... #{e.message}(time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
      puts "Unable to process bulk request: #{e.backtrace.inspect}"
    end
  rescue Exception => e
    @output_filename.error("General Exception:...#{e.message} (time: #{Time.now.to_s}, start time: #{@start_time.to_s})")
    puts "BadRequest: #{e.backtrace.inspect}"
  end
  File.delete output_results
  return json_data
end


CSV.open(@report_filename , 'w') do |csv|
  csv << csv_headers
  csv.close
end
asset_array = []

asset_json = get_bulk_assets()
if !asset_json.nil? then
  asset_json.each do |asset| 
    asset_array << Array[asset.fetch("id"),asset.fetch("hostname"),asset.fetch("ip_address"),asset.fetch("operating_system"),asset.fetch("tags").join(',')]
  end
end

asset_array.each_slice(@assets_per_query.to_i) do |all_assets|
  id_array =[]
  all_assets.each do |one|
    id_array << one[0]
  end
  asset_string = id_array.join('&asset%5Bid%5D%5B%5D=')
  vuln_query = @vuln_url
  vuln_query = "#{vuln_query}&q=#{@q_param}" unless @q_param.nil? || @q_param.empty?
  vuln_query = "#{vuln_query}&asset%5Bid%5D%5B%5D=#{asset_string}"
  vuln_query = "#{vuln_query}&include_details=true" if @include_details
  vuln_pages = 0
  vuln_page = 1
  CSV.open(@report_filename , 'a') do |csv|
    #csv << csv_headers
    vuln_json = JSON.parse(get_data(vuln_query))
    if !vuln_json.nil? then
        vuln_pages = vuln_json["meta"].fetch("pages")
      if vuln_pages > 20 then
        puts "TOO MANY VULNS RERUN WITH A LOWER ASSET COUNT PER BLOCK"
        abort
      end

      while vuln_page <= vuln_pages+1 do 
        puts "vuln pages = #{vuln_pages} and vuln page = #{vuln_page}" if @debug
        vuln_json = JSON.parse(get_data("#{vuln_query}&page=#{vuln_page}"))
        vuln_page_json = vuln_json["vulnerabilities"]
        vuln_page_json.each do |vuln| 
          vuln_id = nil
          status = nil
          cve_id = nil
          score = nil
          cve_description = nil
          created = nil
          last_seen = nil
          qids = []
          fix_id = nil
          cvss_threat = nil
          cvss_severity = nil
          active_breach = nil
          malware_exploitable = nil
          easily_exploitable = nil
          predicted_exploitable = nil
          patch_available = nil
          patch_published = nil
          hostname = nil
          ip = nil
          os = nil
          tags = nil
          details = nil

          vuln_id = vuln.fetch("id")
          status = vuln.fetch("status")
          cve_id = vuln.fetch("cve_id")
          score = vuln.fetch("risk_meter_score")
          cve_description = vuln.fetch("cve_description")
          created = vuln.fetch("created_at")
          last_seen = vuln.fetch("last_seen_time")
          qids = vuln.fetch("identifiers")
          fix_id = vuln.fetch("fix_id")
          cvss_threat = vuln.fetch("threat")
          cvss_severity = vuln.fetch("severity")
          active_breach = vuln.fetch("active_internet_breach")
          malware_exploitable = vuln.fetch("malware_exploitable")
          easily_exploitable = vuln.fetch("easily_exploitable")
          predicted_exploitable = vuln.fetch("predicted_exploitable")
          patch_published = vuln.fetch("patch_published_at")
          if !patch_published.nil? && !patch_published.empty? then
            patch_available = "true"
          else
            patch_available = "false"
          end
          asset_id = vuln.fetch("asset_id")
          a = []
          a = all_assets.find{|a| a[0] == asset_id}
          hostname = a[1]
          ip = a[2]
          os = a[3]
          tags = a[4]
          details = vuln["scanner_vulnerabilities"].first.fetch("details") unless vuln["scanner_vulnerabilities"].first.nil? || !@include_details
          csv_row = []

          csv_row = [vuln_id,
            status,
            cve_id,
            score,
            cve_description,
            created,
            last_seen,
            qids.join(', '),
            fix_id,
            cvss_threat,
            cvss_severity,
            active_breach,
            malware_exploitable,
            easily_exploitable,
            predicted_exploitable,
            patch_available,
            patch_published,
            hostname,
            ip,
            os,
            tags]
            csv_row << details if @include_details
            csv << csv_row

        end
        vuln_page += 1
      end
    end
  end
end



