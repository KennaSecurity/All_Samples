from json.tool import main
import os
import sys
import time
import json
import logging
import requests
import gzip
import shutil
import csv
import math

# Print and log information.
def print_info(msg):
    print(msg)
    logging.info(msg)

# Print and error information.
def print_error(msg):
    print(msg)
    logging.error(msg)

# Process an HTTP error by printing and log.error
def process_http_error(msg, response, url):
    print(f"{msg} HTTP Error: {response.status_code} with {url}")
    if response.text is None:
        logging.error(f"{msg}, {url} status_code: {response.status_code}")
    else:
        logging.error(f"{msg}, {url} status_code: {response.status_code} info: {response.text}")

# Invoke the data_exports API to request an asset export.
def request_asset_exports(base_url, headers):
    request_export_url = f"{base_url}data_exports"

    filter_params = {
        'status' : ['active'],
        "exclude_child_filter" : [
        "include all assets"
    ],
        'export_settings': {
            'format': 'jsonl',
            'model': 'asset'
        }
    }
    
    response = requests.post(request_export_url, headers=headers, data=json.dumps(filter_params))
    if response.status_code != 200:
        process_http_error(f"Request Data Export API Error", response, request_export_url)
        sys.exit(1)

    resp = response.json()
    search_id = str(resp['search_id'])
    num_assets = resp['record_count']
    print_info(f"New search ID: {search_id} with {num_assets} assets")
    return (search_id, num_assets)

def get_export_status(base_url, headers, search_id):
    check_status_url = f"{base_url}data_exports/status?search_id={search_id}"

    response = requests.get(check_status_url, headers=headers)
    if response.status_code == 206:
        return False
    if response.status_code != 200:
        process_http_error(f"Get Export Status API Error", response, check_status_url)
        sys.exit(1)
    
    resp_json = response.json()
    return resp_json['message'] == "Export ready for download"

# Check to see if the export file is ready to download.
def check_export_status(base_url, headers, search_id, num_assets):

    # Check if the export is ready already.
    ready = get_export_status(base_url, headers, search_id)
    if ready:
        return
    
    # Estimate export time for if we're waiting.
    # Calculate wait interval between checking if the export file is ready.
    wait_interval_secs = 5 if num_assets < 2718 else 10
    wait_limit_secs = math.ceil(num_assets / 16)

    # Loop to check status for wait_limit_secs seconds.
    secs = 0
    ready = False
    while not ready and secs < wait_limit_secs:
        print(f"Sleeping for {wait_interval_secs} seconds. ({secs})\r", end='')
        time.sleep(wait_interval_secs)
        ready = get_export_status(base_url, headers, search_id)
        secs += wait_interval_secs 

    print("")
    if secs >= wait_limit_secs:
        print_info(f"Waited for {wait_limit_secs} seconds.")
        print(f"Consider re-running with search ID")
        sys.exit(1)
 
# Obtain the exported asset data and ungzip it.
def retrieve_asset_data(base_url, base_headers, id, asset_file_name):
    jsonl_asset_file_name = f"{asset_file_name}.jsonl"

    if os.path.exists(jsonl_asset_file_name):
        print_info(f"{jsonl_asset_file_name} already exists, so we're using it.")
        return jsonl_asset_file_name

    gz_asset_file_name = f"{asset_file_name}.gz"

    get_data_url = f"{base_url}data_exports/?search_id={id}"
    headers = base_headers.copy()
    headers['Accept'] = "application/gzip; charset=utf-8"
    
    response = requests.get(get_data_url, headers=headers, stream=True)
    if response.status_code != 200:
        process_http_error(f"Retrieve Data Export API Error", response, get_data_url)
        sys.exit(1)
    
    try:
        with open(gz_asset_file_name, 'wb') as file_gz:
            for block in response.iter_content(8192):
                file_gz.write(block)
    
    except Exception as exp:
        print(f"Retrieve asset data error: {str(exp)}")
        logging.error(f"Retrieve asset data error: {str(exp)}")
        sys.exit(1)

    # Gunzip the file into another file.
    print_info(f"Unzipping file {gz_asset_file_name} to {jsonl_asset_file_name}")
    with gzip.open(gz_asset_file_name, 'rb') as f_in:
        with open(jsonl_asset_file_name, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    
    print_info(f"File {gz_asset_file_name} unzipped to {jsonl_asset_file_name}")
    return jsonl_asset_file_name

###End helper method definitions
###Begin main

if __name__ == "__main__":
# See if an ID is passed in.
    logging_file_name = "zero_vuln_assets.log"
    logging.basicConfig(filename=logging_file_name, level=logging.INFO)
    print_info(f"Retrieving asset export...")

    # Process command line arguments.
    id = 0
    try:
        if len(sys.argv) == 2:
            if sys.argv[1] == "-h":
                print_help()
            else:
                id = int(sys.argv[1])
    except ValueError:
        print("Bad string to integer conversion")
        print_help()

# Obtain the Kenna Security API key. Windows instructions: https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/1.5.1/oread/creating-and-modifying-environment-variables-on-windows.html
api_key = os.getenv('KENNA_API_KEY')
if api_key is None:
    print("Environment variable KENNA_API_KEY is non-existent")
    sys.exit(1)

# HTTP headers for Kenna.
headers = {'X-Risk-Token': api_key,
               'Accept': 'application/json',
               'Content-Type': 'application/json; charset=utf-8',
               'User-Agent': 'unique_asset_tags/1.0.0 (Kenna Security)'}

# You might have to change this depending on your deployment.
base_url = "https://api.kennasecurity.com/"

if id == 0:
    (id, num_assets) = request_asset_exports(base_url, headers)
    check_export_status(base_url, headers, id, num_assets)
else:
    print_info(f"Using search ID: {id}")
    check_export_status(base_url, headers, id, 50000)

request_asset_exports(base_url, headers)
check_export_status(base_url, headers, id, 50000)
    
asset_file_name = f"assets_{id}"
jsonl_asset_file_name = retrieve_asset_data(base_url, headers, id, asset_file_name)

# Filter for only assets which have no vulnerabilities
with open(jsonl_asset_file_name, 'rb') as handle:
    input_dict = [json.loads(line) for line in handle]

output_dict = [x for x in input_dict if x['vulnerabilities_count'] == 0]

no_vuln_asset_count = len(output_dict)
print(f"The script found {no_vuln_asset_count} assets with no vulnerabilities.")

with open('no_vulns_assets.json', 'w', encoding='utf-8') as f:
    json.dump(output_dict, f, ensure_ascii=False, indent=4)

asset_id_list = []

for l in output_dict:
    asset_id_list.append(l['id']) 

outfile = open('no_vulns_asset_id_list.csv', 'w', newline='')
out = csv.writer(outfile)
out.writerows(map(lambda x: [x], asset_id_list))
outfile.close
