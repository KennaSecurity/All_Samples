# frozen_string_literal: true

# kenna-bulk-custom-field-update
require 'rest-client'
require 'json'
require 'csv'

# These are the arguments we are expecting to get
@token = ARGV[0]
@assets_per_query = ARGV[1] # number of assets to update at one time to keep the vuln pull under 20 pages
@report_filename = ARGV[2] # name of the resulting csv file
@include_details = ARGV[3] # set to true only if your Kenna instance is enabled for vuln detail extract
@risk_meter_id = ARGV.length > 4 ? ARGV[4] : nil # risk meter from which to pull vulns
@q_param = ARGV.length > 5 ? ARGV[5] : nil
@base_url = ARGV.length > 6 ? ARGV[6] : 'https://api.kennasecurity.com/'

# Variables we'll need later
@vuln_url = "#{@base_url}vulnerabilities/search?per_page=5000"
@rm_url = "#{@base_url}asset_groups/"
@bulk_url = "#{@base_url}data_exports"
@headers = { 'Content-type' => 'application/json', 'X-Risk-Token' => @token }

@max_retries = 5

start_time = Time.now
@output_filename = Logger.new("csv-extract-#{start_time.strftime('%Y%m%dT%H%M')}.txt")
@debug = false

# Encoding characters
enc_colon = '%3A'
enc_dblquote = '%22'
enc_space = '%20'
csv_headers =
  [
    'id',
    'status',
    'cve id',
    'score',
    'cve description',
    'created',
    'last seen',
    'QIDs',
    'fix id',
    'cvss threat',
    'cvss severity',
    'active breach',
    'malware exploitable',
    'easily exploitable',
    'predicted exploitable',
    'patch available',
    'patch published',
    'hostname',
    'ip',
    'os',
    'tags'
  ]
csv_headers << 'details' if @include_details

def get_data(get_url)
  puts 'starting query' if @debug
  puts "get data url = #{get_url}" if @debug
  query_return = ''
  begin
    query_return = RestClient::Request.execute(
      method: :get,
      url: get_url,
      headers: @headers
    )
  rescue RestClient::TooManyRequests => e
    retry
  rescue RestClient::UnprocessableEntity => e
    puts "unprocessible entity: #{e.message}"
  rescue RestClient::BadRequest => e
    @output_filename.error("Rest client BadRequest: #{get_url}...#{e.message} (time: #{Time.now}, start time: #{@start_time})")
    puts "BadRequest: #{e.backtrace.inspect}"
  rescue RestClient::Exception => e
    @retries ||= 0
    if @retries < @max_retries
      @retries += 1
      sleep(15)
      retry
    else
      @output_filename.error("General RestClient error #{get_url}... #{e.message}(time: #{Time.now}, start time: #{@start_time})")
      puts "Unable to get vulns: #{e.backtrace.inspect}"
    end
  rescue Exception => e
    @output_filename.error("General Exception: #{get_url}...#{e.message} (time: #{Time.now}, start time: #{@start_time})")
    puts "BadRequest: #{e.backtrace.inspect}"
  end
  query_return
end

def get_bulk_assets
  puts 'starting bulk query' if @debug
  query_return = ''
  bulk_query_json_string = '{"status": ["active"],'
  bulk_query_json_string += " \"search_id\": \"#{@risk_meter_id}\"," if !@risk_meter_id.nil? && !@risk_meter_id.empty?
  bulk_query_json_string = "#{bulk_query_json_string} \"export_settings\": { \"format\": \"json\", "
  bulk_query_json_string = "#{bulk_query_json_string}\"compression\": \"gzip\", \"model\": \"asset\" }}"

  bulk_query_json = JSON.parse(bulk_query_json_string)

  puts bulk_query_json.to_s if @debug
  begin
    query_response = RestClient::Request.execute(
      method: :post,
      url: @bulk_url,
      headers: @headers,
      payload: bulk_query_json
    )
    puts query_response if @debug
    query_response_json = JSON.parse(query_response.body)
    searchID = query_response_json.fetch('search_id')
    # searchID = 1079331
    puts "searchID = #{searchID}" if @debug
    output_results = "myoutputfile_#{searchID}.gz"
    searchComplete = false

    while searchComplete == false

      status_code = RestClient.get("#{@bulk_url}/status?search_id=#{searchID}", @headers).code

      puts "status code =#{status_code}" if @debug
      if status_code != 200
        puts 'sleeping for async query' if @debug
        sleep(60)
        next
      else
        puts 'ansyc query complete' if @debug
        searchComplete = true
        File.open(output_results, 'w') do |f|
          block = proc { |response|
            response.read_body do |chunk|
              f.write chunk
            end
          }
          RestClient::Request.new(method: :get, url: "#{@bulk_url}?search_id=#{searchID}", headers: @headers,
                                  block_response: block).execute
        end
        gzfile = open(output_results)
        gz = Zlib::GzipReader.new(gzfile)
        json_data = JSON.parse(gz.read)['assets']
      end
    end
  rescue RestClient::TooManyRequests => e
    retry
  rescue RestClient::UnprocessableEntity => e
    puts "unprocessible entity: #{e.message}"
  rescue RestClient::BadRequest => e
    @output_filename.error("Rest client BadRequest:...#{e.message} (time: #{Time.now}, start time: #{@start_time})")
    puts "BadRequest: #{e.backtrace.inspect}"
  rescue RestClient::Exception => e
    @retries ||= 0
    if @retries < @max_retries
      @retries += 1
      sleep(15)
      retry
    else
      @output_filename.error("General RestClient error... #{e.message}(time: #{Time.now}, start time: #{@start_time})")
      puts "Unable to process bulk request: #{e.backtrace.inspect}"
    end
  rescue Exception => e
    @output_filename.error("General Exception:...#{e.message} (time: #{Time.now}, start time: #{@start_time})")
    puts "BadRequest: #{e.backtrace.inspect}"
  end
  File.delete output_results
  json_data
end

CSV.open(@report_filename, 'w') do |csv|
  csv << csv_headers
  csv.close
end
asset_array = []

asset_json = get_bulk_assets
asset_json&.each do |asset|
  asset_array << Array[asset.fetch('id'), asset.fetch('hostname'), asset.fetch('ip_address'),
                       asset.fetch('operating_system'), asset.fetch('tags').join(',')]
end

asset_array.each_slice(@assets_per_query.to_i) do |all_assets|
  id_array = []
  all_assets.each do |one|
    id_array << one[0]
  end
  asset_string = id_array.join('&asset%5Bid%5D%5B%5D=')
  vuln_query = @vuln_url
  vuln_query = "#{vuln_query}&q=#{@q_param}" unless @q_param.nil? || @q_param.empty?
  vuln_query = "#{vuln_query}&asset%5Bid%5D%5B%5D=#{asset_string}"
  vuln_query = "#{vuln_query}&include_details=true" if @include_details
  vuln_pages = 0
  vuln_page = 1
  CSV.open(@report_filename, 'a') do |csv|
    # csv << csv_headers
    vuln_json = JSON.parse(get_data(vuln_query))
    unless vuln_json.nil?
      vuln_pages = vuln_json['meta'].fetch('pages')
      if vuln_pages > 20
        puts 'TOO MANY VULNS RERUN WITH A LOWER ASSET COUNT PER BLOCK'
        abort
      end
      vuln_array = []
      while vuln_page <= vuln_pages + 1
        puts "vuln pages = #{vuln_pages} and vuln page = #{vuln_page}" if @debug
        vuln_json = JSON.parse(get_data("#{vuln_query}&page=#{vuln_page}"))
        vuln_page_json = vuln_json['vulnerabilities']
        vuln_page_json.each do |vuln|
          vuln_id = nil
          status = nil
          cve_id = nil
          score = nil
          cve_description = nil
          cve_published = nil
          created = nil
          last_seen = nil
          qids = []
          fix_id = nil
          cvss_threat = nil
          cvss_severity = nil
          active_breach = nil
          malware_exploitable = nil
          easily_exploitable = nil
          predicted_exploitable = nil
          patch_available = nil
          patch_published = nil
          hostname = nil
          ip = nil
          os = nil
          tags = nil
          details = nil

          vuln_id = vuln.fetch('id')
          status = vuln.fetch('status')
          cve_id = vuln.fetch('cve_id')
          score = vuln.fetch('risk_meter_score')
          cve_description = vuln.fetch('cve_description')
          created = vuln.fetch('created_at')
          last_seen = vuln.fetch('last_seen_time')
          qids = vuln.fetch('identifiers')
          fix_id = vuln.fetch('fix_id')
          cvss_threat = vuln.fetch('threat')
          cvss_severity = vuln.fetch('severity')
          active_breach = vuln.fetch('active_internet_breach')
          malware_exploitable = vuln.fetch('malware_exploitable')
          easily_exploitable = vuln.fetch('easily_exploitable')
          predicted_exploitable = vuln.fetch('predicted_exploitable')
          patch_published = vuln.fetch('patch_published_at')
          patch_available = if !patch_published.nil? && !patch_published.empty?
                              'true'
                            else
                              'false'
                            end
          asset_id = vuln.fetch('asset_id')
          a = []
          a = all_assets.find { |a| a[0] == asset_id }
          hostname = a[1]
          ip = a[2]
          os = a[3]
          tags = a[4]
          unless vuln['scanner_vulnerabilities'].first.nil? || !@include_details
            details = vuln['scanner_vulnerabilities'].first.fetch('details')
          end
          cvs_row = []

          csv_row = [vuln_id,
                     status,
                     cve_id,
                     score,
                     cve_description,
                     created,
                     last_seen,
                     qids.join(', '),
                     fix_id,
                     cvss_threat,
                     cvss_severity,
                     active_breach,
                     malware_exploitable,
                     easily_exploitable,
                     predicted_exploitable,
                     patch_available,
                     patch_published,
                     hostname,
                     ip,
                     os,
                     tags]
          csv_row << details if @include_details
          csv << csv_row
        end
        vuln_page += 1
      end
    end
  end
end
